// src/hooks/useChat.js - Ollama Optimized
import { useState, useCallback } from 'react';

export const useChat = () => {
  const [messages, setMessages] = useState([
    {
      id: 1,
      type: 'ai',
      content: 'à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š! à¸œà¸¡à¸„à¸·à¸­ AI Assistant à¸ªà¸³à¸«à¸£à¸±à¸šà¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ Access Log \n\nà¸œà¸¡à¸—à¸³à¸‡à¸²à¸™à¸”à¹‰à¸§à¸¢ Ollama Local Model à¹à¸¥à¸°à¸ªà¸²à¸¡à¸²à¸£à¸–à¸Šà¹ˆà¸§à¸¢à¸„à¸¸à¸“à¹„à¸”à¹‰à¹ƒà¸™à¹€à¸£à¸·à¹ˆà¸­à¸‡:\n\nðŸ“Š à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹à¸™à¸§à¹‚à¸™à¹‰à¸¡à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡\nðŸ” à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸žà¸¤à¸•à¸´à¸à¸£à¸£à¸¡à¸œà¸´à¸”à¸›à¸à¸•à¸´  \nðŸ“‹ à¸ªà¸£à¸¸à¸›à¸ªà¸–à¸´à¸•à¸´à¹à¸¥à¸°à¸£à¸²à¸¢à¸‡à¸²à¸™\nâ“ à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥\n\nà¸¡à¸µà¸­à¸°à¹„à¸£à¹ƒà¸«à¹‰à¸Šà¹ˆà¸§à¸¢à¹„à¸«à¸¡à¸„à¸£à¸±à¸š?',
      timestamp: new Date()
    }
  ]);
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState(null);

  // Ollama configuration
  const ollamaUrl = import.meta.env.VITE_OLLAMA_URL || 'http://localhost:11434';
  const ollamaModel = import.meta.env.VITE_OLLAMA_MODEL || 'llama3.2:3b';
  const ollamaTimeout = parseInt(import.meta.env.VITE_OLLAMA_TIMEOUT) || 30000;
  const maxRetries = parseInt(import.meta.env.VITE_OLLAMA_MAX_RETRIES) || 3;
  const debugMode = import.meta.env.VITE_DEBUG_AI === 'true';

  // Check Ollama availability
  const checkOllamaStatus = async () => {
    try {
      const response = await fetch(`${ollamaUrl}/api/tags`, {
        method: 'GET',
        signal: AbortSignal.timeout(5000) // 5 second timeout for health check
      });
      
      if (!response.ok) {
        throw new Error(`Ollama server returned ${response.status}`);
      }
      
      const data = await response.json();
      const availableModels = data.models || [];
      const hasModel = availableModels.some(model => model.name === ollamaModel);
      
      if (debugMode) {
        console.log('ðŸ¤– Ollama Status:', {
          available: true,
          models: availableModels.map(m => m.name),
          targetModel: ollamaModel,
          hasTargetModel: hasModel
        });
      }
      
      return {
        available: true,
        hasModel,
        availableModels: availableModels.map(m => m.name)
      };
    } catch (error) {
      if (debugMode) {
        console.error('âŒ Ollama not available:', error.message);
      }
      return {
        available: false,
        error: error.message
      };
    }
  };

  // Optimized Ollama API call with retry logic
  const callOllamaAPI = async (prompt, retryCount = 0) => {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), ollamaTimeout);

    try {
      if (debugMode) {
        console.log(`ðŸ¤– Calling Ollama (attempt ${retryCount + 1}):`, {
          model: ollamaModel,
          promptLength: prompt.length,
          timeout: ollamaTimeout
        });
      }

      const response = await fetch(`${ollamaUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: ollamaModel,
          prompt: prompt,
          stream: false,
          options: {
            temperature: 0.7,
            top_p: 0.9,
            top_k: 40,
            num_predict: 1000, // Maximum tokens to generate
            num_ctx: 4096,     // Context window size
            repeat_penalty: 1.1
          }
        }),
        signal: controller.signal
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        throw new Error(`Ollama API Error: ${response.status} - ${response.statusText}`);
      }

      const data = await response.json();
      
      if (debugMode) {
        console.log('âœ… Ollama Response:', {
          responseLength: data.response?.length || 0,
          totalDuration: data.total_duration,
          loadDuration: data.load_duration,
          promptEvalCount: data.prompt_eval_count,
          evalCount: data.eval_count
        });
      }

      return data.response || 'à¸‚à¸­à¸­à¸ à¸±à¸¢à¸„à¸£à¸±à¸š à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸•à¸­à¸šà¹„à¸”à¹‰';

    } catch (error) {
      clearTimeout(timeoutId);
      
      if (error.name === 'AbortError') {
        throw new Error(`à¸„à¸³à¸‚à¸­à¸«à¸¡à¸”à¹€à¸§à¸¥à¸² (${ollamaTimeout / 1000} à¸§à¸´à¸™à¸²à¸—à¸µ)`);
      }
      
      // Retry logic
      if (retryCount < maxRetries && error.message.includes('fetch')) {
        console.warn(`ðŸ”„ Retry ${retryCount + 1}/${maxRetries} for Ollama call`);
        await new Promise(resolve => setTimeout(resolve, 1000 * (retryCount + 1))); // Exponential backoff
        return callOllamaAPI(prompt, retryCount + 1);
      }
      
      throw error;
    }
  };

  // Create system prompt for Thai Access Log context
  const createSystemPrompt = (userMessage) => {
    return `à¸„à¸¸à¸“à¹€à¸›à¹‡à¸™ AI Assistant à¸œà¸¹à¹‰à¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¸”à¹‰à¸²à¸™à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ Access Log à¸£à¸°à¸šà¸šà¹€à¸‚à¹‰à¸²à¸­à¸­à¸à¸­à¸²à¸„à¸²à¸£ à¸„à¸¸à¸“à¸¡à¸µà¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š:

ðŸ“Š à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ Access Log
ðŸ” à¸„à¸§à¸²à¸¡à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢à¹à¸¥à¸°à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¸žà¸¤à¸•à¸´à¸à¸£à¸£à¸¡à¸œà¸´à¸”à¸›à¸à¸•à¸´  
ðŸ“ˆ à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡à¸£à¸²à¸¢à¸‡à¸²à¸™à¹à¸¥à¸°à¸ªà¸–à¸´à¸•à¸´
ðŸ¢ à¸£à¸°à¸šà¸šà¸„à¸§à¸šà¸„à¸¸à¸¡à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¸­à¸²à¸„à¸²à¸£

à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™à¹ƒà¸™à¸£à¸°à¸šà¸š:
- à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: 143,898 à¸£à¸²à¸¢à¸à¸²à¸£
- à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆà¸«à¸¥à¸±à¸: à¸ªà¸³à¸™à¸±à¸à¸‡à¸²à¸™à¹ƒà¸«à¸à¹ˆ à¸­à¸²à¸„à¸²à¸£ 1 (à¸™à¸²à¸™à¸²à¹€à¸«à¸™à¸·à¸­) à¸Šà¸±à¹‰à¸™ 5
- à¸›à¸£à¸°à¹€à¸ à¸—à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰: EMPLOYEE, VISITOR, AFFILIATE
- à¸—à¸´à¸¨à¸—à¸²à¸‡: IN (à¹€à¸‚à¹‰à¸²), OUT (à¸­à¸­à¸)

à¸à¸£à¸¸à¸“à¸²à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸—à¸µà¹ˆà¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸‡à¹ˆà¸²à¸¢ à¹ƒà¸«à¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œ à¹à¸¥à¸°à¹ƒà¸Šà¹‰ emoji à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸™à¹ˆà¸²à¸ªà¸™à¹ƒà¸ˆ

à¸„à¸³à¸–à¸²à¸¡: ${userMessage}

à¸„à¸³à¸•à¸­à¸š:`;
  };

  // Send message handler with Ollama integration
  const handleSendMessage = useCallback(async (messageContent) => {
    // Validate input
    if (!messageContent || typeof messageContent !== 'string') {
      setError('à¸à¸£à¸¸à¸“à¸²à¹ƒà¸ªà¹ˆà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸ªà¹ˆà¸‡');
      return;
    }

    const trimmedMessage = messageContent.trim();
    if (!trimmedMessage) {
      setError('à¸à¸£à¸¸à¸“à¸²à¹ƒà¸ªà¹ˆà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸ªà¹ˆà¸‡');
      return;
    }

    // Clear any previous errors
    setError(null);

    // Add user message
    const userMessage = {
      id: Date.now(),
      type: 'user',
      content: trimmedMessage,
      timestamp: new Date()
    };

    setMessages(prev => [...prev, userMessage]);
    setIsLoading(true);

    try {
      // Check Ollama status first
      const ollamaStatus = await checkOllamaStatus();
      
      if (!ollamaStatus.available) {
        throw new Error(`à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­ Ollama à¹„à¸”à¹‰: ${ollamaStatus.error}\n\nà¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸² Ollama à¸—à¸³à¸‡à¸²à¸™à¸­à¸¢à¸¹à¹ˆà¸—à¸µà¹ˆ ${ollamaUrl}`);
      }
      
      if (!ollamaStatus.hasModel) {
        throw new Error(`à¹„à¸¡à¹ˆà¸žà¸šà¹‚à¸¡à¹€à¸”à¸¥ "${ollamaModel}"\n\nà¹‚à¸¡à¹€à¸”à¸¥à¸—à¸µà¹ˆà¸¡à¸µà¸­à¸¢à¸¹à¹ˆ: ${ollamaStatus.availableModels.join(', ')}\n\nà¹ƒà¸Šà¹‰à¸„à¸³à¸ªà¸±à¹ˆà¸‡: ollama pull ${ollamaModel}`);
      }

      // Create system prompt
      const systemPrompt = createSystemPrompt(trimmedMessage);
      
      // Call Ollama API
      const aiResponse = await callOllamaAPI(systemPrompt);

      // Add AI response
      const aiMessage = {
        id: Date.now() + 1,
        type: 'ai',
        content: aiResponse,
        timestamp: new Date(),
        model: ollamaModel
      };

      setMessages(prev => [...prev, aiMessage]);

    } catch (error) {
      console.error('âŒ Ollama response error:', error);
      
      // Add error message with helpful information
      const errorMessage = {
        id: Date.now() + 1,
        type: 'ai',
        content: `âŒ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”: ${error.message}

ðŸ”§ à¸§à¸´à¸˜à¸µà¹à¸à¹‰à¹„à¸‚:
1. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸² Ollama à¸—à¸³à¸‡à¸²à¸™à¸­à¸¢à¸¹à¹ˆ: \`ollama serve\`
2. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹‚à¸¡à¹€à¸”à¸¥: \`ollama list\`
3. à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹‚à¸¡à¹€à¸”à¸¥à¸–à¹‰à¸²à¸ˆà¸³à¹€à¸›à¹‡à¸™: \`ollama pull ${ollamaModel}\`
4. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š URL: ${ollamaUrl}

ðŸ’¡ à¸«à¸£à¸·à¸­à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹„à¸›à¹ƒà¸Šà¹‰ Mock AI à¸Šà¸±à¹ˆà¸§à¸„à¸£à¸²à¸§à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ .env:
\`VITE_AI_PROVIDER=mock\``,
        timestamp: new Date(),
        isError: true
      };

      setMessages(prev => [...prev, errorMessage]);
      setError('à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸•à¸´à¸”à¸•à¹ˆà¸­ Ollama');
    } finally {
      setIsLoading(false);
    }
  }, [ollamaUrl, ollamaModel, ollamaTimeout, maxRetries, debugMode]);

  // Clear messages
  const clearMessages = useCallback(() => {
    setMessages([
      {
        id: 1,
        type: 'ai',
        content: `à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š! à¸œà¸¡à¸žà¸£à¹‰à¸­à¸¡à¸Šà¹ˆà¸§à¸¢à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ Access Log à¹à¸¥à¹‰à¸§ ðŸ¤–

ðŸ”§ **à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸°à¸šà¸š:**
- à¹‚à¸¡à¹€à¸”à¸¥: ${ollamaModel}
- Ollama URL: ${ollamaUrl}
- Debug Mode: ${debugMode ? 'à¹€à¸›à¸´à¸”' : 'à¸›à¸´à¸”'}

à¸¡à¸µà¸­à¸°à¹„à¸£à¹ƒà¸«à¹‰à¸Šà¹ˆà¸§à¸¢à¹„à¸«à¸¡à¸„à¸£à¸±à¸š?`,
        timestamp: new Date()
      }
    ]);
    setError(null);
  }, [ollamaModel, ollamaUrl, debugMode]);

  // Clear error
  const clearError = useCallback(() => {
    setError(null);
  }, []);

  // Test Ollama connection
  const testOllamaConnection = useCallback(async () => {
    setIsLoading(true);
    try {
      const status = await checkOllamaStatus();
      const testMessage = {
        id: Date.now(),
        type: 'ai',
        content: `ðŸ§ª **à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š Ollama:**

ðŸ“¡ **à¸à¸²à¸£à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­:** ${status.available ? 'âœ… à¸ªà¸³à¹€à¸£à¹‡à¸ˆ' : 'âŒ à¸¥à¹‰à¸¡à¹€à¸«à¸¥à¸§'}
ðŸ¤– **à¹‚à¸¡à¹€à¸”à¸¥à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢:** ${ollamaModel}
ðŸŽ¯ **à¹‚à¸¡à¹€à¸”à¸¥à¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰:** ${status.hasModel ? 'âœ… à¸žà¸£à¹‰à¸­à¸¡' : 'âŒ à¹„à¸¡à¹ˆà¸žà¸š'}

ðŸ“‹ **à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸µà¹ˆà¸¡à¸µà¹ƒà¸™à¸£à¸°à¸šà¸š:**
${status.availableModels ? status.availableModels.map(model => `â€¢ ${model}`).join('\n') : 'à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥'}

${!status.available ? `\nâ— **à¸„à¸³à¹à¸™à¸°à¸™à¸³:**\n1. à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™ Ollama: \`ollama serve\`\n2. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š URL: ${ollamaUrl}` : ''}
${status.available && !status.hasModel ? `\nâ— **à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹‚à¸¡à¹€à¸”à¸¥:** \`ollama pull ${ollamaModel}\`` : ''}`,
        timestamp: new Date()
      };
      setMessages(prev => [...prev, testMessage]);
    } catch (error) {
      console.error('Connection test failed:', error);
    } finally {
      setIsLoading(false);
    }
  }, [ollamaUrl, ollamaModel]);

  return {
    messages,
    isLoading,
    error,
    handleSendMessage,
    clearMessages,
    clearError,
    testOllamaConnection,
    aiProvider: 'ollama',
    modelInfo: {
      name: ollamaModel,
      url: ollamaUrl,
      timeout: ollamaTimeout,
      debug: debugMode
    }
  };
};